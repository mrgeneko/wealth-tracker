PHASE 9 - METRICS COMPATIBILITY & INTEGRATION
==============================================

OVERVIEW
========
Phase 9's analytics system is designed to handle and visualize ALL metrics currently
generated by the scrapers, dashboard, and Phase 8 metrics system. This document maps
existing metrics to Phase 9 analytics endpoints and charts.

EXISTING METRICS INVENTORY
===========================

1. SCRAPER METRICS (scraper_utils.js)
   Currently generated but not persisted:
   
   - totalNavigations: Number of page navigations attempted
   - failedNavigations: Number of failed page navigations
   - totalRequests: Number of HTTP requests made
   - failedRequests: Number of failed HTTP requests
   - Status: Logged to console, not stored in database
   
   Phase 9 Action: Store in symbol_registry_metrics table

2. PHASE 8 METRICS (symbol_registry_metrics table)
   Already persisted:
   
   - metric_date: Date of metrics entry
   - source: File source (NASDAQ_FILE, NYSE_FILE, OTHER_FILE, TREASURY_FILE)
   - total_symbols: Count of symbols loaded
   - symbols_with_yahoo_metadata: Count with metadata
   - symbols_without_yahoo_metadata: Count without metadata
   - last_file_refresh_at: Timestamp of last refresh
   - file_download_duration_ms: Time to download file
   - avg_yahoo_fetch_duration_ms: Average Yahoo API fetch time
   - errors_count: Number of errors during operation
   
   Phase 9 Action: Use directly for analytics dashboards

3. FILE REFRESH STATUS (file_refresh_status table)
   Already persisted:
   
   - file_type: NASDAQ, NYSE, OTHER, TREASURY
   - last_refresh_at: Timestamp of last refresh
   - last_refresh_duration_ms: How long the refresh took
   - last_refresh_status: SUCCESS, FAILED, IN_PROGRESS
   - last_error_message: Error details if failed
   - symbols_added: Count of new symbols
   - symbols_updated: Count of updated symbols
   - next_refresh_due_at: When next refresh is scheduled
   
   Phase 9 Action: Use directly for refresh analytics

4. SYMBOL REGISTRY STATISTICS (symbol_registry table)
   Already persisted:
   
   - security_type: EQUITY, ETF, BOND, TREASURY
   - has_yahoo_metadata: 0/1 flag
   - sort_rank: Rank for sorting by priority
   - source: Where symbol came from
   - Various financial fields (dividend, P/E ratio, etc.)
   
   Phase 9 Action: Aggregate for analytics charts

4. DASHBOARD SCHEDULER METRICS (dashboard frontend)
   Currently tracked in localStorage:
   
   - lastRefresh: When metadata refresh was last run
   - nextRefresh: When next refresh is scheduled
   - totalSymbolsNeedingMetadata: Count awaiting data
   - completionPercentage: % of symbols with metadata
   
   Phase 9 Action: Persist to database, include in analytics

5. SCRAPER PAGE PERFORMANCE METRICS (Not currently tracked)
   NEW in Phase 9 - Per-page and per-navigation tracking:
   
   - navigation_url: URL of the page loaded
   - navigation_duration_ms: Time to load page (gotoWithRetries)
   - scrape_duration_ms: Time to extract data from page
   - page_size_bytes: Size of HTML content
   - elements_found: Number of data elements scraped
   - retry_count: How many retries needed for navigation
   - retry_total_duration_ms: Total time spent on retries
   - errors_on_page: Count of errors during scrape
   - source: Which scraper generated the data (robinhood, cnbc, etc.)
   
   Phase 9 Action: Track at page level, aggregate by source

MISSING METRICS TO ADD
======================

These metrics would enhance Phase 9 analytics but don't currently exist:

1. Dashboard Activity Metrics
   - User actions (search, filter, refresh)
   - Page view counts
   - Feature usage statistics
   - Error counts (client-side)
   
2. Performance Metrics
   - API response times (per endpoint)
   - Database query times
   - Chart rendering times
   - Memory usage

3. Data Quality Metrics
   - Symbol deduplication rate
   - Malformed data count
   - Data validation failure rate
   - Inconsistent data points

4. Business Metrics
   - Symbol registry size over time
   - Metadata coverage by type
   - Data freshness (how old last refresh was)
   - User satisfaction (implied from error rates)

PHASE 9 INTEGRATION PLAN
========================

STEP 1: EXTEND SCRAPER METRICS PERSISTENCE
───────────────────────────────────────────

Current: Scrapers generate metrics but only log to console
Future: Store all scraper metrics in database

New table: scraper_performance_metrics
├── metric_date DATE
├── source VARCHAR(50) - Scraper type
├── total_navigations INT
├── failed_navigations INT
├── total_requests INT
├── failed_requests INT
├── success_rate DECIMAL(5,2)
├── duration_ms INT
└── created_at TIMESTAMP

Changes to: scrapers/scraper_utils.js
```javascript
// Add function to persist metrics
async function persistScraperMetrics(pool, source, metrics) {
  const conn = await pool.getConnection();
  try {
    await conn.execute(`
      INSERT INTO scraper_performance_metrics 
      (metric_date, source, total_navigations, failed_navigations, 
       total_requests, failed_requests, success_rate, duration_ms)
      VALUES (?, ?, ?, ?, ?, ?, ?, ?)
    `, [
      new Date(),
      source,
      metrics.totalNavigations,
      metrics.failedNavigations,
      metrics.totalRequests,
      metrics.failedRequests,
      calculateSuccessRate(metrics),
      metrics.duration || 0
    ]);
  } finally {
    conn.release();
  }
}
```

STEP 2: EXTEND FILE REFRESH STATUS TABLE
─────────────────────────────────────────

Current: Tracks basic refresh status
Future: Add more detailed metrics

Add columns to file_refresh_status:
├── symbols_skipped INT
├── symbols_with_errors INT
├── file_size_bytes INT
├── records_in_file INT
└── refresh_duration_seconds INT

STEP 3: CREATE SCHEDULER METRICS TABLE
──────────────────────────────────────

Current: Scheduler metrics only in localStorage
Future: Persist to database for historical tracking

New table: scheduler_metrics
├── id INT AUTO_INCREMENT PRIMARY KEY
├── metric_date DATE
├── scheduled_task VARCHAR(100)
├── execution_timestamp DATETIME
├── duration_ms INT
├── symbols_processed INT
├── symbols_with_metadata INT
├── symbols_without_metadata INT
├── completion_percentage DECIMAL(5,2)
├── status VARCHAR(20)
└── error_message TEXT (nullable)

STEP 4: CREATE SCRAPER PAGE PERFORMANCE TABLE
──────────────────────────────────────────────

Current: Scrapers track only aggregate metrics
Future: Track individual page navigation and scrape times

New table: scraper_page_performance
├── id INT AUTO_INCREMENT PRIMARY KEY
├── scraper_source VARCHAR(50) - Which scraper (robinhood, cnbc, webull, etc.)
├── page_url VARCHAR(500) - URL of page being scraped
├── navigation_start_time DATETIME
├── navigation_duration_ms INT - Time to load page via gotoWithRetries
├── retry_count INT - How many retries for this navigation
├── retry_total_duration_ms INT - Total time spent retrying
├── navigation_success BOOLEAN - Did navigation succeed
├── scrape_start_time DATETIME
├── scrape_duration_ms INT - Time to extract data from page
├── page_size_bytes INT - Size of loaded HTML
├── elements_found INT - Number of data elements scraped
├── errors_count INT - Errors during this page's scrape
├── error_details TEXT - Error messages
├── created_at TIMESTAMP
└── metric_date DATE - For easy daily aggregation

Query: Track per-scraper and per-source

STEP 4.5: AGGREGATION TABLE FOR DAILY SUMMARIES
─────────────────────────────────────────────────

New table: scraper_daily_summary
├── metric_date DATE
├── scraper_source VARCHAR(50)
├── total_pages_scraped INT
├── total_navigation_time_ms INT
├── avg_navigation_time_ms INT
├── min_navigation_time_ms INT
├── max_navigation_time_ms INT
├── total_scrape_time_ms INT
├── avg_scrape_time_ms INT
├── total_retry_count INT
├── success_rate_percentage DECIMAL(5,2)
├── total_errors INT
├── avg_page_size_bytes INT
└── PRIMARY KEY (metric_date, scraper_source)

STEP 4: CREATE DASHBOARDS FOR EACH METRIC TYPE
───────────────────────────────────────────────

Phase 9.1 Analytics API will provide:

A. SYMBOL REGISTRY ANALYTICS
   GET /api/analytics/symbol-growth
   - Uses: symbol_registry table
   - Shows: Total symbols over time (by date)
   - Can filter by: source, security_type
   
   GET /api/analytics/type-distribution
   - Uses: symbol_registry table
   - Shows: EQUITY/ETF/BOND/TREASURY pie chart
   
   GET /api/analytics/metadata-completion
   - Uses: symbol_registry table
   - Shows: % with Yahoo metadata

B. FILE REFRESH ANALYTICS
   GET /api/analytics/refresh-success-rate
   - Uses: file_refresh_status table
   - Shows: Success % by file type
   
   GET /api/analytics/sync-performance
   - Uses: file_refresh_status table
   - Shows: Sync duration trends
   
   GET /api/analytics/error-trends
   - Uses: file_refresh_status table (new error tracking)
   - Shows: Error count over time

C. SCRAPER PERFORMANCE ANALYTICS (New in Phase 9)
   GET /api/analytics/scraper-performance
   - Uses: scraper_performance_metrics table (new)
   - Shows: Navigation/request success rates
   
   GET /api/analytics/scraper-reliability
   - Uses: scraper_performance_metrics table
   - Shows: Success rate by scraper type

D. SCHEDULER ANALYTICS (New in Phase 9)
   GET /api/analytics/scheduler-performance
   - Uses: scheduler_metrics table (new)
   - Shows: Metadata refresh progress over time
   
   GET /api/analytics/metadata-population-progress
   - Uses: scheduler_metrics table
   - Shows: Completion % trend

METRICS FLOW DIAGRAM
====================

```
Scrapers
├─ scraper_utils.js generates metrics
├─ NOW: Logged to console
├─ PHASE 9: Persist to scraper_performance_metrics table
└─ PHASE 9: Visualize in analytics dashboard

Symbol Registry Sync
├─ symbol_registry_sync.js generates metrics
├─ NOW: Stored in file_refresh_status table
├─ NOW: Stored in symbol_registry_metrics table
└─ PHASE 9: Visualize in analytics dashboard

Metadata Population
├─ MetadataRefreshScheduler generates metrics
├─ NOW: Stored in localStorage (frontend only)
├─ PHASE 9: Persist to scheduler_metrics table
└─ PHASE 9: Visualize in analytics dashboard

Database Tables
├─ symbol_registry_metrics (Phase 8, used in Phase 9)
├─ file_refresh_status (Phase 8, used in Phase 9)
├─ scraper_performance_metrics (NEW in Phase 9)
└─ scheduler_metrics (NEW in Phase 9)

Analytics Endpoints (Phase 9)
├─ /api/analytics/symbol-growth
├─ /api/analytics/metadata-completion
├─ /api/analytics/refresh-success-rate
├─ /api/analytics/sync-performance
├─ /api/analytics/error-trends (Enhanced)
├─ /api/analytics/type-distribution
├─ /api/analytics/scraper-performance (NEW)
├─ /api/analytics/scraper-reliability (NEW)
├─ /api/analytics/scheduler-performance (NEW)
└─ /api/analytics/metadata-population-progress (NEW)

Dashboards (Phase 9)
├─ analytics.html - Symbol registry charts
├─ refresh-analytics.html - File refresh charts
├─ scraper-analytics.html - Scraper performance (NEW)
├─ scheduler-analytics.html - Metadata population (NEW)
└─ health-analytics.html - System health composite
```

IMPLEMENTATION DETAILS
======================

PHASE 9.1: Extend Existing Metrics
───────────────────────────────────

Files to modify:
1. scrapers/scraper_utils.js
   - Add persistScraperMetrics() function
   - Call after each scrape operation
   - Store in database instead of just logging

2. database schema (new migration script)
   - Create scraper_performance_metrics table
   - Create scheduler_metrics table
   - Add columns to file_refresh_status

3. services/symbol-registry/symbol_registry_sync.js
   - Already records metrics (Phase 8)
   - Just ensure data is formatted for Phase 9 analytics

PHASE 9.2: Analytics API Endpoints
───────────────────────────────────

Create /api/analytics.js with endpoints for:
- Symbol registry metrics
- File refresh metrics
- Scraper performance metrics (new)
- Scheduler metrics (new)
- Error analysis
- Trend analysis

All endpoints return standardized format:
```json
{
  "labels": ["Date1", "Date2", ...],
  "datasets": [
    {
      "label": "Dataset Name",
      "data": [100, 200, 150, ...],
      "color": "#3b82f6"
    }
  ]
}
```

PHASE 9.3: Dashboard Pages
───────────────────────────

Create analytics pages:
1. analytics.html - Symbol registry health
   ├── Symbol growth line chart
   ├── Type distribution pie chart
   ├── Metadata completion gauge
   └── Source breakdown table

2. refresh-analytics.html - Refresh operations
   ├── Success rate by file type
   ├── Sync performance trends
   ├── Error analysis
   └── File status table

3. scraper-analytics.html (NEW - DETAILED SCRAPER METRICS)
   ├── Navigation time by source (bar chart)
   │   └── Robinhood vs CNBC vs Webull vs MarketBeat
   ├── Scrape time by source (bar chart)
   │   └── Per-source average scrape duration
   ├── Pages processed per source (line chart)
   │   └── How many pages each scraper handles
   ├── Navigation success rate (gauge by source)
   │   └── % of navigations that succeed per scraper
   ├── Scraper reliability comparison (radar chart)
   │   └── All metrics per source: speed, success rate, error rate
   ├── Navigation retry analysis (scatter plot)
   │   └── Relationship between retry count and duration
   ├── Error rate by source (bar chart)
   │   └── Which scraper has most errors
   └── Top slow pages (table)
       └── Pages that take longest to load or scrape

4. scheduler-analytics.html (NEW)
   ├── Metadata population progress
   ├── Completion percentage trend
   ├── Symbols processed per run
   └── Schedule execution history

5. health-analytics.html - System overview
   ├── Overall health score
   ├── All metrics in one view
   ├── Alert status
   └── Recommendations

LIVE DASHBOARD UPDATES
======================

Phase 9 will support LIVE chart updates through WebSocket or polling:

Option 1: Polling (Simpler, Phase 9.1)
──────────────────────────────────────
- Dashboard polls /api/analytics/* endpoints every 5-30 seconds
- Charts update automatically with fresh data
- No server-side changes needed beyond API endpoints
- Trade-off: Slight latency (5-30s delay), small bandwidth

Implementation:
```javascript
class LiveAnalyticsDashboard extends AnalyticsDashboard {
  constructor(adapterClass, refreshIntervalMs = 10000) {
    super(adapterClass);
    this.refreshIntervalMs = refreshIntervalMs;
    this.updateTimers = {};
  }

  startLiveUpdates() {
    // Start polling each chart every refreshIntervalMs
    Object.entries(this.charts).forEach(([chartName, chart]) => {
      this.updateTimers[chartName] = setInterval(
        () => chart.refresh(),
        this.refreshIntervalMs
      );
    });
  }

  stopLiveUpdates() {
    Object.values(this.updateTimers).forEach(timer => clearInterval(timer));
    this.updateTimers = {};
  }
}

// Usage:
const liveDashboard = new LiveAnalyticsDashboard(ChartJsAdapter, 10000); // 10s refresh
await liveDashboard.initialize();
liveDashboard.startLiveUpdates();
```

Option 2: WebSocket (Real-time, Phase 9.2+)
────────────────────────────────────────────
- Dashboard establishes WebSocket connection
- Server pushes updates as metrics are generated
- Real-time updates (sub-second latency)
- More server resources required
- Better user experience for fast-changing metrics

Implementation would require:
- WebSocket server endpoint: /ws/analytics
- Metrics broadcast to connected clients when updated
- Chart adapters handle incremental updates
- Graceful reconnection handling

SCRAPER PERFORMANCE: SOURCE-BY-SOURCE TRACKING
===============================================

Phase 9 WILL track each scraper separately:

Per-Source Tracking:
├── Robinhood scraper
│   ├── Pages navigated: 50
│   ├── Avg navigation time: 2.3s
│   ├── Avg scrape time: 0.8s
│   ├── Success rate: 98%
│   └── Errors: 1
│
├── CNBC scraper
│   ├── Pages navigated: 100
│   ├── Avg navigation time: 3.1s
│   ├── Avg scrape time: 1.2s
│   ├── Success rate: 95%
│   └── Errors: 5
│
├── Webull scraper
│   ├── Pages navigated: 75
│   ├── Avg navigation time: 2.8s
│   ├── Avg scrape time: 1.0s
│   ├── Success rate: 96%
│   └── Errors: 3
│
└── MarketBeat scraper
    ├── Pages navigated: 60
    ├── Avg navigation time: 2.5s
    ├── Avg scrape time: 0.9s
    ├── Success rate: 97%
    └── Errors: 2

Example API Response:
GET /api/analytics/scraper-performance?days=7
```json
{
  "labels": ["Day 1", "Day 2", "Day 3", ...],
  "datasets": [
    {
      "label": "Robinhood - Navigation Time (ms)",
      "data": [2300, 2400, 2250, ...],
      "borderColor": "#FF6B6B"
    },
    {
      "label": "CNBC - Navigation Time (ms)",
      "data": [3100, 3200, 3050, ...],
      "borderColor": "#4ECDC4"
    },
    {
      "label": "Webull - Navigation Time (ms)",
      "data": [2800, 2900, 2750, ...],
      "borderColor": "#45B7D1"
    },
    {
      "label": "MarketBeat - Navigation Time (ms)",
      "data": [2500, 2600, 2450, ...],
      "borderColor": "#96CEB4"
    }
  ]
}
```

CHART UPDATES
=============

Live Update Strategy:

1. HISTORICAL CHARTS (Daily aggregates)
   └── Update every 24 hours (no need for frequent refresh)
   └── Example: Symbol growth, metadata completion trend

2. OPERATIONAL METRICS (Per-run aggregates)
   └── Update every 10-30 minutes (after each refresh/run)
   └── Example: Refresh success rate, sync performance

3. REAL-TIME METRICS (Per-page data)
   └── Update as scrapers run (5-10 second latency acceptable)
   └── Example: Current page being scraped, navigation in progress
   └── Requires WebSocket for true real-time (Phase 9.2+)

Default Phase 9.1 (Polling):
```javascript
// Different charts refresh at different intervals
chartsConfig = {
  'symbolGrowth': { refreshInterval: 3600000 }, // 1 hour
  'metadataCompletion': { refreshInterval: 600000 }, // 10 minutes
  'refreshSuccessRate': { refreshInterval: 300000 }, // 5 minutes
  'scraperPerformance': { refreshInterval: 60000 }, // 1 minute
  'currentScraperStatus': { refreshInterval: 5000 } // 5 seconds (if polling)
};
```

DATA AGGREGATION QUERIES
========================

Example queries Phase 9 will use:

Symbol Growth Over Time:
```sql
SELECT 
  DATE(metric_date) as date,
  SUM(total_symbols) as count
FROM symbol_registry_metrics
GROUP BY DATE(metric_date)
ORDER BY date DESC
LIMIT 30;
```

Refresh Success Rate by Type:
```sql
SELECT 
  file_type,
  COUNT(*) as total_refreshes,
  SUM(CASE WHEN last_refresh_status = 'SUCCESS' THEN 1 ELSE 0 END) as successful,
  ROUND(SUM(CASE WHEN last_refresh_status = 'SUCCESS' THEN 1 ELSE 0 END) / COUNT(*) * 100, 2) as success_rate
FROM file_refresh_status
WHERE last_refresh_at >= DATE_SUB(NOW(), INTERVAL 30 DAY)
GROUP BY file_type;
```

Scraper Performance Trends:
```sql
SELECT 
  DATE(metric_date) as date,
  source,
  AVG(total_navigations) as avg_navigations,
  AVG(failed_navigations) as avg_failures,
  AVG(duration_ms) as avg_duration_ms
FROM scraper_performance_metrics
GROUP BY DATE(metric_date), source
ORDER BY date DESC, source;
```

Metadata Population Progress:
```sql
SELECT 
  DATE(execution_timestamp) as date,
  SUM(symbols_with_metadata) as with_metadata,
  SUM(symbols_without_metadata) as without_metadata,
  MAX(completion_percentage) as completion_pct
FROM scheduler_metrics
GROUP BY DATE(execution_timestamp)
ORDER BY date DESC
LIMIT 30;
```

COMPATIBILITY MATRIX
====================

Existing Metrics → Phase 9 Analytics

Scraper Metrics
├─ totalNavigations → /api/analytics/scraper-performance (per-source)
├─ failedNavigations → /api/analytics/scraper-reliability (per-source)
├─ totalRequests → /api/analytics/scraper-performance
├─ failedRequests → /api/analytics/scraper-reliability
├─ NEW: page_load_time (per page) → /api/analytics/scraper-page-breakdown
├─ NEW: scrape_duration (per page) → /api/analytics/scraper-page-breakdown
├─ NEW: retry_count (per navigation) → /api/analytics/scraper-retry-analysis
└─ NEW: error_details (per page) → /api/analytics/scraper-error-analysis

Phase 8 Metrics
├─ symbol_registry_metrics → /api/analytics/symbol-growth
├─ symbol_registry_metrics → /api/analytics/metadata-completion
├─ file_refresh_status → /api/analytics/refresh-success-rate
├─ file_refresh_status → /api/analytics/sync-performance
├─ file_refresh_status → /api/analytics/error-trends
└─ symbol_registry → /api/analytics/type-distribution

Dashboard Scheduler
├─ lastRefresh → /api/analytics/scheduler-performance (per-source)
├─ completionPercentage → /api/analytics/metadata-population-progress
├─ totalSymbolsNeedingMetadata → /api/analytics/metadata-population-progress
└─ nextRefresh → /api/analytics/scheduler-performance

Scraper Page Performance (NEW)
├─ Per-page navigation time → /api/analytics/scraper-performance (aggregated by source)
├─ Per-page scrape time → /api/analytics/scraper-performance (aggregated by source)
├─ Retry analysis → /api/analytics/scraper-retry-analysis
├─ Error breakdown → /api/analytics/scraper-error-analysis
├─ Success rates by source → /api/analytics/scraper-reliability
└─ Slowest pages table → /api/analytics/scraper-performance-detail

PHASE 9 NEW TABLES REQUIRED
=============================

1. scraper_performance_metrics
   - Tracks: Navigation/request success, duration
   - Updated by: scraper_utils.js (Phase 9 enhanced)
   - Used by: /api/analytics/scraper-* endpoints

2. scheduler_metrics
   - Tracks: Metadata population progress
   - Updated by: MetadataRefreshScheduler (Phase 9 enhanced)
   - Used by: /api/analytics/scheduler-* endpoints

3. scraper_page_performance (NEW)
   - Tracks: Per-page navigation time, scrape time, retry count
   - Updated by: gotoWithRetries wrapper, scraper operations
   - Used by: /api/analytics/scraper-performance, scraper-reliability endpoints
   - Granularity: Individual page (high volume, needs pruning)
   - Retention: Last 7 days of detailed data
   
4. scraper_daily_summary (NEW)
   - Tracks: Aggregated daily metrics per scraper source
   - Updated by: Nightly aggregation job from scraper_page_performance
   - Used by: Historical trend charts
   - Granularity: Per source per day
   - Retention: All historical data

IMPLEMENTATION: TRACKING PER PAGE
==================================

In scraper_utils.js, wrap gotoWithRetries to track timing:

```javascript
// Enhanced gotoWithRetries wrapper
async function gotoWithRetriesTracked(
  page, 
  url, 
  options = {}, 
  maxAttempts = 3,
  scraperSource = 'unknown'
) {
  const startTime = Date.now();
  let retryCount = 0;
  let lastError = null;
  
  for (let attempt = 1; attempt <= maxAttempts; attempt++) {
    const navigationStart = Date.now();
    try {
      const res = await page.goto(url, options);
      const navigationDuration = Date.now() - navigationStart;
      
      // Store navigation performance
      if (global.metricsCollector) {
        await global.metricsCollector.recordPageNavigation({
          scraperSource,
          pageUrl: url,
          navigationDurationMs: navigationDuration,
          retryCount: attempt - 1,
          navigationSuccess: true
        });
      }
      
      return res;
    } catch (err) {
      retryCount++;
      lastError = err;
      if (attempt < maxAttempts) {
        await new Promise(resolve => setTimeout(resolve, 1000 * attempt));
      }
    }
  }
  
  // Record failed navigation
  if (global.metricsCollector) {
    await global.metricsCollector.recordPageNavigation({
      scraperSource,
      pageUrl: url,
      navigationDurationMs: Date.now() - startTime,
      retryCount,
      navigationSuccess: false,
      errorMessage: lastError?.message
    });
  }
  
  throw lastError;
}

// Then in each scraper:
const pageOpts = { 
  url, 
  downloadPath: outputDir, 
  waitUntil: 'domcontentloaded', 
  timeout: 20000, 
  gotoRetries: 3 
};

const scrapeStart = Date.now();
await gotoWithRetriesTracked(page, url, pageOpts, 3, 'robinhood');

// ... extract data ...

const scrapeDuration = Date.now() - scrapeStart;
await global.metricsCollector.recordPageScrape({
  scraperSource: 'robinhood',
  pageUrl: url,
  scrapeDurationMs: scrapeDuration,
  elementsFound: extractedData.length,
  pageSizeBytes: (await page.content()).length
});
```

METRICS COLLECTION SERVICE
===========================

New service: services/metrics/ScraperMetricsCollector.js

```javascript
class ScraperMetricsCollector {
  constructor(pool) {
    this.pool = pool;
    this.batchSize = 100;
    this.pendingMetrics = [];
  }

  async recordPageNavigation(data) {
    this.pendingMetrics.push({
      type: 'navigation',
      data
    });
    if (this.pendingMetrics.length >= this.batchSize) {
      await this.flush();
    }
  }

  async recordPageScrape(data) {
    this.pendingMetrics.push({
      type: 'scrape',
      data
    });
    if (this.pendingMetrics.length >= this.batchSize) {
      await this.flush();
    }
  }

  async flush() {
    if (this.pendingMetrics.length === 0) return;
    
    const conn = await this.pool.getConnection();
    try {
      const navigationMetrics = this.pendingMetrics
        .filter(m => m.type === 'navigation')
        .map(m => m.data);
      
      const scrapeMetrics = this.pendingMetrics
        .filter(m => m.type === 'scrape')
        .map(m => m.data);

      // Batch insert navigations
      if (navigationMetrics.length > 0) {
        await conn.query(
          `INSERT INTO scraper_page_performance 
           (scraper_source, page_url, navigation_start_time, 
            navigation_duration_ms, retry_count, navigation_success, created_at)
           VALUES ?`,
          [navigationMetrics.map(m => [
            m.scraperSource, m.pageUrl, new Date(), 
            m.navigationDurationMs, m.retryCount, 
            m.navigationSuccess, new Date()
          ])]
        );
      }

      // Batch insert scrapes
      if (scrapeMetrics.length > 0) {
        // Similar batch insert for scrape metrics
      }

      this.pendingMetrics = [];
    } finally {
      conn.release();
    }
  }

  async generateDailySummary(date) {
    const conn = await this.pool.getConnection();
    try {
      // Aggregate previous day's data into scraper_daily_summary
      await conn.query(`
        INSERT INTO scraper_daily_summary
        SELECT 
          DATE(created_at) as metric_date,
          scraper_source,
          COUNT(*) as total_pages_scraped,
          SUM(navigation_duration_ms) as total_navigation_time_ms,
          AVG(navigation_duration_ms) as avg_navigation_time_ms,
          MIN(navigation_duration_ms) as min_navigation_time_ms,
          MAX(navigation_duration_ms) as max_navigation_time_ms,
          AVG(scrape_duration_ms) as avg_scrape_time_ms,
          SUM(retry_count) as total_retry_count,
          ROUND(SUM(CASE WHEN navigation_success THEN 1 ELSE 0 END) / 
                COUNT(*) * 100, 2) as success_rate_percentage,
          SUM(errors_count) as total_errors,
          AVG(page_size_bytes) as avg_page_size_bytes
        FROM scraper_page_performance
        WHERE DATE(created_at) = ?
        GROUP BY DATE(created_at), scraper_source
      `, [date]);
      
      // Optionally prune old detailed metrics (keep last 7 days)
      await conn.query(`
        DELETE FROM scraper_page_performance 
        WHERE created_at < DATE_SUB(NOW(), INTERVAL 7 DAY)
      `);
    } finally {
      conn.release();
    }
  }
}
```

DASHBOARD: SOURCE-BY-SOURCE COMPARISON
=======================================

Charts will show per-source metrics:

1. Navigation Time Comparison
   ├── X-axis: Date/Time
   ├── Y-axis: Navigation time (ms)
   ├── Lines: One per scraper (Robinhood, CNBC, Webull, MarketBeat)
   └── Shows: Which scraper is slowest and when
   
2. Success Rate Scorecard
   ├── Robinhood: 98% ✓
   ├── CNBC: 95% ⚠️
   ├── Webull: 96% ✓
   └── MarketBeat: 97% ✓

3. Error Analysis
   ├── X-axis: Scraper source
   ├── Y-axis: Error count
   ├── Color: Error severity
   └── Shows: Which scraper needs attention

4. Performance Radar Chart
   ├── Each scraper is a spoke
   ├── Metrics: Speed, Success Rate, Reliability, Data Volume
   └── Visual comparison of all sources at once

TIMELINE & EFFORT
=================

Database Schema Updates:        1-2 days
Scraper Metrics Persistence:   1-2 days
Analytics API Endpoints:        3-4 days
Dashboard Pages & Charts:       3-4 days
Integration & Testing:          2-3 days
─────────────────────────────────────
Total Additional Effort:       10-15 days

This extends the original Phase 9 estimate by ~1-2 days, bringing total to ~12-16 days.

SUMMARY
=======

✅ Phase 9 WILL handle ALL existing metrics:
   - Scraper metrics (will be persisted)
   - Phase 8 metrics (already persisted)
   - File refresh metrics (already persisted)
   - Scheduler metrics (will be persisted)
   - Symbol registry stats (already available)

✅ Phase 9 analytics is DESIGNED for:
   - Multiple data sources
   - Easy metric addition
   - Flexible querying
   - Various chart types

✅ Metrics collection is ENHANCED:
   - Scrapers now persist metrics
   - Scheduler metrics tracked long-term
   - Better error tracking
   - Performance profiling

✅ Charts AUTOMATICALLY update with:
   - New metrics as added
   - Historical trends visible
   - Comparisons across metrics
   - Alerting potential (future phase)
